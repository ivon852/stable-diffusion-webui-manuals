---
title: "外部链接"
weight: 1
---

本手册的参考来源

- [AUTOMATIC1111/stable-diffusion-webui Wiki - Github](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/)
- [Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10684-10695).](https://arxiv.org/abs/2112.10752)
- [Diffusers - Hugging Face](https://huggingface.co/docs/diffusers/index)
- [Voldy's Retard Guide The definitive Stable Diffusion experience ™](https://rentry.org/voldy)
- [Stable Diffusion Art - tutorials, prompts, resources](https://stable-diffusion-art.com)

Stable Diffusion讨论社群

- [r/StableDiffusion - Reddit](https://www.reddit.com/r/StableDiffusion/)
- [看板AI_Art 文章列表- 批踢踢实业坊](https://www.ptt.cc/bbs/AI_Art/index.html)
- [Stable Diffusion 台湾社群v2 - Facebook](https://www.facebook.com/groups/sdaitw/)
- [Stable Diffusion - 知乎](https://www.zhihu.com/topic/26072993)


模型训练参考数据

- [Hypernetwork training for dummies - Rentry.co](https://rentry.co/hypernetwork4dumdums)
- [【Novel AI】手把手教你Textual Inversion神经网络训练详细操作](https://www.bilibili.com/read/cv19088146)
- [NovelAI hypernetwork 自训练教程 - 知乎专栏](https://zhuanlan.zhihu.com/p/576041621)
- [Basic Dreambooth Guide - Github](https://github.com/nitrosocke/dreambooth-training-guide)
- [LoRA Training Guide - Rentry.co](https://rentry.org/lora_train)
- [Using LoRA for Efficient Stable Diffusion Fine-Tuning - HuggingFace](https://huggingface.co/blog/lora)
- [曾飞飞 - 零基础读懂Stable Diffusion](https://zhuanlan.zhihu.com/p/597247221)
- [Well-Researched Comparison of Training Techniques Lora, Inversion, Dreambooth, Hypernetworks](https://www.reddit.com/r/StableDiffusion/comments/10cgxrx/wellresearched_comparison_of_training_techniques/)
- [【AI绘画】当我们在训练AI的时候，究竟在训练什幺？AI训练前置知识（一）](https://www.bilibili.com/read/cv19249573)
- [【笔记】Stable Diffusion模型优化入门级介绍 - LoRA vs Dreambooth vs Textural Inversion vs Hypernetworks](https://zhuanlan.zhihu.com/p/612992813)
- [Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.](https://arxiv.org/abs/2106.09685)

